{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a9823a0-0763-4a1e-a29a-401cb1ac4593",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## MOUNTING S3 BUCKET ON DATABRICKS FILE SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f88e5a3-8d31-47fd-a731-f5f8dca24dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c520de-7413-4f84-8209-3b0184b5984f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the parth\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "#read the path to spark dataframe\n",
    "access_key_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128d7d0b-94cf-43bf-a207-06a47b1e4160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACCESS_KEY =access_key_df.select('Access key ID').collect()[0]['Access key ID'] # Store the Access Key ID obtained from the DataFrame.\n",
    "SECRET_KEY =access_key_df.select('Secret access key').collect()[0]['Secret access key'] #  Store the Secret Access Key from the DataFrame.\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\") # Store the URL-encoded Secret Access Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c6c65d-2ed9-44db-8252-d16230e8623f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "aws_bucket_name = \"user-0e4753f224a7-bucket\" # Define AWS bucket name.\n",
    "mounted_name = \"pinterest_data_new\" # Define mounted directory name.\n",
    "dbutils.fs.mount(f\"s3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{aws_bucket_name}\", f\"/mnt/s3_data/{mounted_name}\") #Mount S3 bucket using Access Key, Secret Key, and bucket name.\n",
    "display(dbutils.fs.ls(f\"/mnt/s3_data/{mounted_name}\")) # Display the contents of the mounted directory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46ce04e-da1a-40fd-911c-ce52e990d391",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CLEANING OF PIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ce8bab-0103-4f70-9648-14cd1500ff28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cleaning pin data\n",
    "def Cleaning_of_pin_data(data):\n",
    "    # Rename Index column to ind\n",
    "    df_pin = data.withColumnRenamed('Index', 'ind')\n",
    "\n",
    "    # Selecting required columns\n",
    "    df_pin1 = df_pin.select(['ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category'])\n",
    "\n",
    "    # Clean follower_count, title and poster_name columns\n",
    "    df_pin1 = df_pin1.withColumn('follower_count', regexp_replace(col('follower_count'), 'k', ' '))\n",
    "    df_pin1 = df_pin1.withColumn('poster_name', regexp_replace(col('poster_name'), '[^a-zA-Z0-9]', ' '))\n",
    "    df_pin1 = df_pin1.withColumn('title', regexp_replace(col('title'), '[^a-zA-Z0-9]', ' '))\n",
    "\n",
    "    # Cast follower_count and ind columns to IntegerType\n",
    "    df_pin1 = df_pin1.withColumn('follower_count', col('follower_count').cast(IntegerType()))\n",
    "    df_pin1 = df_pin1.withColumn('ind', col('ind').cast(IntegerType()))\n",
    "\n",
    "    # Filter rows where poster_name is not 'User Info Error' and follower_count is not null\n",
    "    df_pin1 = df_pin1.filter((col('poster_name') != 'User Info Error') & (col('follower_count').isNotNull()))\n",
    "\n",
    "    # Adjust save_location column\n",
    "    df_pin_batch = df_pin1.withColumn('save_location', col('save_location').substr(14, 100))\n",
    "    return df_pin_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922e05d6-587a-4da2-aa1d-c5c7adc4ea07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CLEANING OF GEO DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e786d49-f98f-46d6-9f17-89acb12b1510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array, col, to_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def cleaning_of_geo_data(data2):\n",
    "    # Reading the geo data from dbfs\n",
    "\n",
    "    # Create 'coordinates' column as an array of 'latitude' and 'longitude'\n",
    "    df_geo1 = data2.withColumn('coordinates', array(col('latitude'), col('longitude')))\n",
    "\n",
    "    # Drop 'latitude' and 'longitude' columns\n",
    "    df_geo1 = df_geo1.drop('latitude', 'longitude')\n",
    "\n",
    "    # Convert 'timestamp' column to timestamp type\n",
    "    df_geo1 = df_geo1.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "    # Rename 'index' column to 'ind'\n",
    "    df_geo1 = df_geo1.withColumnRenamed(\"index\", 'ind')\n",
    "\n",
    "    # Cast 'ind' column to IntegerType\n",
    "    df_geo1 = df_geo1.withColumn('ind', col('ind').cast(IntegerType()))\n",
    "\n",
    "    # Select specific columns\n",
    "    df_geo_batch = df_geo1.select('ind', 'country', 'coordinates', 'timestamp')\n",
    "    return df_geo_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cea53a-4451-40c7-bf13-b5962c02e393",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## CLEANING USER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9393ed6e-e4bd-49c0-a4df-c04d173fa63b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit, date_format, to_date\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def cleaning_of_user_data(data3):\n",
    "    # Concatenating 'first_name' and 'last_name' into a new column 'user_name'\n",
    "    df_user1 = data3.withColumn('user_name', concat(data3['first_name'], lit(' '), data3['last_name']))\n",
    "\n",
    "    # Dropping 'first_name' and 'last_name' columns from the DataFrame\n",
    "    df_user1 = df_user1.drop('first_name', 'last_name')\n",
    "\n",
    "    # Formatting 'date_joined' column to \"yyyy-MM-dd\" format\n",
    "    df_user1 = df_user1.withColumn('date_joined', date_format(df_user1['date_joined'], \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Converting formatted 'date_joined' column to DateType\n",
    "    df_user1 = df_user1.withColumn('date_joined', to_date(df_user1['date_joined']))\n",
    "\n",
    "    # Renaming 'index' column to 'ind'\n",
    "    df_user1 = df_user1.withColumnRenamed('index', 'ind')\n",
    "\n",
    "    # Casting 'ind' column to IntegerType\n",
    "    df_user1 = df_user1.withColumn('ind', df_user1['ind'].cast(IntegerType()))\n",
    "\n",
    "    # Casting 'age' column to IntegerType\n",
    "    df_user1 = df_user1.withColumn('age', df_user1['age'].cast(IntegerType()))\n",
    "\n",
    "    # Selecting specific columns ('ind', 'user_name', 'age', 'date_joined') from the DataFrame\n",
    "    df_user1 = df_user1.select('ind', 'user_name', 'age', 'date_joined')\n",
    "    return df_user1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data from DBFS and Calling the fucntions to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e03ebf-7ef4-4b10-adc0-a40ed94b41d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_geo_batch = spark.read.json('dbfs:/mnt/data/pinterest_data_new/topics/0e4753f224a7.geo/partition=0/*.json', multiLine=True)\n",
    "df_user_batch = spark.read.json('dbfs:/mnt/data/pinterest_data_new/topics/0e4753f224a7.user/partition=0/*.json', multiLine=True)\n",
    "df_pin_batch = spark.read.json('dbfs:/mnt/data/pinterest_data_new/topics/0e4753f224a7.pin/partition=0/*.json', multiLine=True)\n",
    "df_pin_batch_clean = Cleaning_of_pin_data(df_pin_batch)\n",
    "df_geo_batch_clean = cleaning_of_geo_data(df_geo_batch)\n",
    "df_user_batch_clean = cleaning_of_user_data(df_user_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the cleaned data to parquet table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2023c1c-d7f4-454a-97a8-02ce94dc4345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pin_batch_clean.write.format('parquet').save('/mnt/data/pinterest_data_new/batch_datapin1/')\n",
    "df_geo_batch_clean.write.format('parquet').save('/mnt/data/pinterest_data_new/batch_datageo1/')\n",
    "df_user_batch_clean.write.format('parquet').save('/mnt/data/pinterest_data_new/batch_datauser1/')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4162440541817933,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_data_processing_in_databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
