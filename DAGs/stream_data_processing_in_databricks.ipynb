{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting ASW Kinesis to Databrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# Define the parth\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "#read the path to spark dataframe\n",
    "access_key_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Retriving the access key and secret keys\n",
    "ACCESS_KEY =access_key_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY =access_key_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for the pin table\n",
    "schema_pin = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", IntegerType()),\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for the user table\n",
    "schema_user = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"date_joined\", DateType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema for geo table\n",
    "schema_geo = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing cleaning functions from batch_data_process\n",
    "%run \"/Users/ugwuegbe@gmail.com/batch_data_processing_in_databricks\"\n",
    "#cleaning_of_pin_data(df_pin)\n",
    "#cleaning_of_geo_data(df_geo)\n",
    " #cleaning_of_user_data(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Stream data from AWS Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pin data from kinesis using spark\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0e4753f224a7-pin') \\\n",
    ".option('initialPosition','latest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code creates a new DataFrame df_pin from the existing df_pin DataFrame.\n",
    "df_pin_stream = df_pin \\\n",
    "    .withColumn(\"jsonData\", df_pin[\"data\"].cast(\"string\")) \\\n",
    "    .withColumn(\"parsedJson\", from_json(\"jsonData\", schema_pin)) \\\n",
    "    .select(\"parsedJson.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of pin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the imported pin data cleaning function to clean the streaming data\n",
    "df_pin_stream = cleaning_of_pin_data(df_pin_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the cleaned stream pin data to delta table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Pin data to DBFS\n",
    "df_pin_stream.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0e4753f224a7_pin_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read geo data from kinesis using spark\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0e4753f224a7-geo') \\\n",
    ".option('initialPosition','latest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code creates a new DataFrame df_geo from the existing df_geo DataFrame.\n",
    "\n",
    "df_geo_stream = df_geo \\\n",
    "    .withColumn(\"jsonData\", df_geo[\"data\"].cast(\"string\")) \\\n",
    "    .withColumn(\"parsedJson\", from_json(\"jsonData\", schema_geo)) \\\n",
    "    .select(\"parsedJson.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of geo Stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the imported geo data cleaning function to clean the streaming geo data\n",
    "df_geo_stream = cleaning_of_geo_data(df_geo_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writ the cleaned stream geo data to delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the geo data to DBFS\n",
    "df_geo1.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0e4753f224a7_geo_tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read user data from kinesis using spark\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-0e4753f224a7-user') \\\n",
    ".option('initialPosition','latest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of code creates a new DataFrame df_user_stream from the existing df_user DataFrame.\n",
    "\n",
    "df_user_stream = df_user \\\n",
    "    .withColumn(\"jsonData\", df_user[\"Data\"].cast(\"string\")) \\\n",
    "    .withColumn(\"parsedJson\", from_json(\"jsonData\", schema=schema_user)) \\\n",
    "    .select(\"parsedJson.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of user stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the imported user data cleaning function to clean the streaming user data\n",
    "df_user_stream = cleaning_of_user_data(df_user_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the cleaned streaming user data to delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the user data to DBFS\n",
    "df_user1.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"0e4753f224a7_user_table\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
