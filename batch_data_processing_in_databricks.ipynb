{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOUNTING S3 ON DATABRICKS FILE SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# URL processing\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parth\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "#read the path to spark dataframe\n",
    "access_key_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriving the access key and secret keys\n",
    "ACCESS_KEY =access_key_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY =access_key_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "aws_bucket_name = \"user-0e4753f224a7-bucket\"\n",
    "mount_name = \"pinterest_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount the s3 to DBFS\n",
    "dbutils.fs.mount(f\"s3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
    "display(dbutils.fs.ls(f\"/mnt/{mount_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the pin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_of_pin_data(data):\n",
    "\n",
    "    # Rename column 'index' to 'ind' in DataFrame df_pin\n",
    "    df_pin = data.withColumnRenamed('index', 'ind')\n",
    "\n",
    "    # Select specific columns from DataFrame df_pin and assign to df1\n",
    "    df1 = df_pin.select(['ind', 'unique_id', 'title', 'description', 'follower_count', 'poster_name', 'tag_list', 'is_image_or_video', 'image_src', 'save_location', 'category'])\n",
    "\n",
    "    # Remove 'k' from 'follower_count' column in DataFrame df_pin1\n",
    "    df_pin1 = df1.withColumn('follower_count', regexp_replace(col('follower_count'), 'k', \" \"))\n",
    "\n",
    "    # Remove non-alphanumeric characters from 'poster_name' column in DataFrame df_pin1\n",
    "    df_pin1 = df_pin1.withColumn('poster_name', regexp_replace(col('poster_name'), '[^a-zA-Z0-9]', \" \"))\n",
    "\n",
    "    # Convert 'follower_count' column to IntegerType in DataFrame df_pin1\n",
    "    df_pin1 = df_pin1.withColumn('follower_count', col('follower_count').cast(IntegerType()))\n",
    "\n",
    "    # Convert 'ind' column to IntegerType in DataFrame df_pin1\n",
    "    df_pin1 = df_pin1.withColumn('ind', col('ind').cast(IntegerType()))\n",
    "\n",
    "    # Filter DataFrame df_pin1 to remove rows where 'poster_name' is 'User Info Error' and 'follower_count' is null\n",
    "    df_pin1 = df_pin1.filter((col('poster_name') != 'User Info Error') & (col('follower_count').isNotNull()))\n",
    "\n",
    "    # Extract substring of length 100 starting from index 14 in 'save_location' column of DataFrame df_pin1\n",
    "    df_pin1 = df_pin1.withColumn('save_location', col('save_location').substr(14, 100))\n",
    "    #reorder the column to follow the prescribed format\n",
    "    df_pin_batch = df_pin1.select(['ind','unique_id','title','description','follower_count','poster_name','tag_list','is_image_or_video','image_src','save_location','category'])\n",
    "    return df_pin_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_of_geo_data(data)\n",
    "    # Create 'coordinates' column as an array of 'latitude' and 'longitude'\n",
    "    df_geo1 = data.withColumn('coordinates', array(col('latitude'), col('longitude')))\n",
    "\n",
    "    # Drop 'latitude' and 'longitude' columns\n",
    "    df_geo1 = df_geo1.drop('latitude', 'longitude')\n",
    "\n",
    "    # Convert 'timestamp' column to timestamp type\n",
    "    df_geo1 = df_geo1.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd'T'HH:mm:ss\"))\n",
    "\n",
    "    # Rename 'index' column to 'ind'\n",
    "    df_geo1 = df_geo1.withColumnRenamed(\"index\", 'ind')\n",
    "\n",
    "    # Cast 'ind' column to IntegerType\n",
    "    df_geo1 = df_geo1.withColumn('ind', col('ind').cast(IntegerType()))\n",
    "\n",
    "    # Select specific columns\n",
    "    df_geo_batch = df_geo1.select('ind', 'country', 'coordinates', 'timestamp')\n",
    "    return df_geo_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_of_user_data(data):\n",
    "    # Concatenate 'first_name' and 'last_name' columns with a space in between and create a new column 'user_name'\n",
    "    df_user1 = data.withColumn('user_name', concat(data['first_name'], lit(' '), data['last_name']))\n",
    "\n",
    "    # Drop 'first_name' and 'last_name' columns from the DataFrame\n",
    "    df_user1 = df_user1.drop('first_name', 'last_name')\n",
    "\n",
    "    # Format 'date_joined' column to the \"yyyy-MM-dd\" format\n",
    "    df_user1 = df_user1.withColumn('date_joined', date_format(df_user1.date_joined, \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Convert formatted 'date_joined' column to DateType\n",
    "    df_user1 = df_user1.withColumn('date_joined', to_date(df_user1.date_joined))\n",
    "\n",
    "    # Rename 'index' column to 'ind'\n",
    "    df_user1 = df_user1.withColumnRenamed('index', 'ind')\n",
    "\n",
    "    # Cast 'ind' column to IntegerType\n",
    "    df_user1 = df_user1.withColumn('ind', col('ind').cast(IntegerType()))\n",
    "\n",
    "    # Cast 'age' column to IntegerType\n",
    "    df_user1 = df_user1.withColumn('age', col('age').cast(IntegerType()))\n",
    "\n",
    "    # Select specific columns 'ind', 'user_name', 'age', and 'date_joined' from the DataFrame\n",
    "    df_user_batch = df_user1.select('ind', 'user_name', 'age', 'date_joined')\n",
    "    return df_user_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data (pin, geo, and user) from DBSF and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Json files from the DBFS \n",
    "df_pin = spark.read.json('dbfs:/mnt/pinterest_data/topics/0e4753f224a7.pin/partition=0/*.json', multiLine=True)\n",
    "\n",
    "# Reading geo data from DBFS\n",
    "df_geo = spark.read.json('mnt/pinterest_data/topics/0e4753f224a7.geo/partition=0/*.json', multiLine=True)\n",
    "\n",
    "#Read Json files from the DBFS \n",
    "df_user = spark.read.json('mnt/pinterest_data/topics/0e4753f224a7.user/partition=0/*.json', multiLine=True)\n",
    "\n",
    "# Calling the functions for pin, geo and user cleaning\n",
    "df_pin_batch = cleaning_of_pin_data(df_pin)\n",
    "df_geo_batch = cleaning_of_geo_data(df_geo)\n",
    "df_user_batch = cleaning_of_user_data(df_user)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
